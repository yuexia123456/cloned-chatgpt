import streamlit as st

from langchain.memory import ConversationBufferMemory
from utils import get_chat_response

st.title("ğŸ’¬å…‹éš†ChatGPT")

with st.sidebar:
    api_key = st.text_input("è¯·è¾“å…¥ä½ çš„apiå¯†é’¥", type="password")
    st.markdown("[ç”³è¯·ä½ çš„apiå¯†é’¥](https://www.aliyun.com/product/bailian)")
    restart = st.button("æ–°å»ºå¯¹è¯")

if restart:
    st.session_state.clear()

if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(return_messages=True)
    st.session_state["messages"] = [{"role": "ai",
                                     "content": "ä½ å¥½,æˆ‘æ˜¯ä½ çš„aiåŠ©æ‰‹,æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—?"}]
for message in st.session_state["messages"]:
    st.chat_message(message["role"]).write(message["content"])

prompt = st.chat_input()
if prompt:
    if not api_key:
        st.info("è¯·è¾“å…¥ä½ çš„apiå¯†é’¥")
        st.stop()
    st.session_state["messages"].append({"role": "human", "content": prompt})
    st.chat_message("human").write(prompt)

    with st.spinner("AIæ­£åœ¨æ€è€ƒä¸­,è¯·ç¨ç­‰..."):
        response = get_chat_response(prompt,st.session_state["memory"], "qwen-plus",
                          api_key,base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
    msg = {"role": "ai", "content": response}
    st.session_state["messages"].append(msg)
    st.chat_message("ai").write(response)